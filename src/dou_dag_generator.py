"""
Dynamic DAG generator with YAML config system to create DAG which
searches terms in the Gazzete [Diário Oficial da União-DOU] and send it
by email to the  provided `recipient_emails` list. The DAGs are
generated by YAML config files at `dag_confs` folder.

TODO:
[] - Definir sufixo do título do email a partir de configuração
"""

import ast
import logging
import os
import sys
import textwrap
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union
import json

import pandas as pd
from airflow import DAG, Dataset
from airflow.utils.task_group import TaskGroup
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.slack.notifications.slack import SlackNotifier
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable


sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
from utils.date import get_trigger_date, template_ano_mes_dia_trigger_local_time
from notification.notifier import Notifier
from parsers import DAGConfig, YAMLParser
from searchers import BaseSearcher, DOUSearcher, QDSearcher, INLABSSearcher


SearchResult = Dict[str, Dict[str, List[dict]]]


def merge_results(result_1: SearchResult, result_2: SearchResult) -> SearchResult:
    """Merge search results by group and term as keys"""
    return {
        group: _merge_dict(result_1.get(group, {}), result_2.get(group, {}))
        for group in set((*result_1, *result_2))
    }


def _merge_dict(dict1, dict2):
    """Merge dictionaries and sum values of common keys"""
    dict3 = {**dict1, **dict2}
    for key, value in dict3.items():
        if key in dict1 and key in dict2:
            dict3[key] = value + dict1[key]
    return dict3


def result_as_html(specs: DAGConfig) -> bool:
    """Só utiliza resultado HTML apenas para email"""
    return specs.report.discord["webhook"] and specs.report.slack["webhook"]


class DouDigestDagGenerator:
    """
    YAML based Generator of DAGs that digests the DOU (gazette) daily
    publication and send email report listing all documents matching
    pré-defined keywords. It's also possible to fetch keywords from the
    database.
    """

    YAMLS_DIR = os.getenv("RO_DOU__DAG_CONF_DIR")

    if YAMLS_DIR is None:
        raise EnvironmentError("Environment variable RO_DOU__DAG_CONF_DIR not found!")

    YAMLS_DIR_LIST = [dag_confs for dag_confs in YAMLS_DIR.split(":")]
    SLACK_CONN_ID = "slack_notify_rodou_dagrun"
    DEFAULT_SCHEDULE = "0 5 * * *"

    parser = YAMLParser
    searchers: Dict[str, BaseSearcher]

    def __init__(self):
        self.searchers = {
            "DOU": DOUSearcher(),
            "QD": QDSearcher(),
            "INLABS": INLABSSearcher(),
        }
        try:
            conn = BaseHook.get_connection(self.SLACK_CONN_ID)
            description = json.loads(conn.description)
            slack_notifier = SlackNotifier(
                slack_conn_id=self.SLACK_CONN_ID,
                text=(
                    ":bomb:"
                    "\n`DAG`  {{ ti.dag_id }}"
                    "\n`State`  {{ ti.state }}"
                    "\n`Task`  {{ ti.task_id }}"
                    "\n`Execution`  {{ ti.execution_date }}"
                    "\n`Log`  {{ ti.log_url }}"
                ),
                channel=description["channel"],
            )
        except Exception as e:
            logging.info("Connection to DAG run notifier not configured: %s", str(e))
            slack_notifier = None

        self.on_failure_callback = slack_notifier
        self.on_retry_callback = None

    @staticmethod
    def prepare_doc_md(specs: DAGConfig, config_file: str) -> str:
        """Prepares the markdown documentation for a dag.

        Args:
            specs (DAGConfig): A DAG configuration object.
            config_file (str): The name of a DAG config file.

        Returns:
            str: The DAG documentation in markdown format.
        """
        config = specs.model_dump()
        # options that won't show in the "DAG Docs"
        del config["description"]
        del config["doc_md"]
        doc_md = specs.doc_md + textwrap.dedent(
            f"""

            **Configuração da dag definida no arquivo `{config_file}`**:

            <dl>
            """
        )
        for key, value in config.items():
            doc_md = doc_md + f"<dt>{key}</dt>"
            if isinstance(value, list) or isinstance(value, set):
                doc_md = doc_md + (
                    "<dd>\n\n"
                    + " * "
                    + "\n * ".join(str(item) for item in value)
                    + "\n</dd>"
                )
            else:
                doc_md = doc_md + f"<dd>{str(value)}</dd>"
            doc_md = doc_md + "\n"
        doc_md = doc_md + "</dl>\n"
        return doc_md

    @staticmethod
    def _hash_dag_id(dag_id: str, size: int) -> int:
        """Hashes the `dag_id` into a integer between 0 and `size`"""
        buffer = 0
        for _char in dag_id:
            buffer += ord(_char)
        try:
            _hash = buffer % size
        except ZeroDivisionError:
            raise ValueError("`size` deve ser maior que 0.")
        return _hash

    def _get_safe_schedule(self, specs: DAGConfig, default_schedule: str) -> str:
        """Retorna um novo valor de `schedule` randomizando o
        minuto de execução baseado no `dag_id`, caso a dag utilize o
        schedule padrão. Aplica uma função de hash na string
        dag_id que retorna valor entre 0 e 60 que define o minuto de
        execução.
        """

        schedule = default_schedule
        id_based_minute = self._hash_dag_id(specs.id, 60)
        schedule_without_min = " ".join(schedule.split(" ")[1:])
        schedule = f"{id_based_minute} {schedule_without_min}"

        return schedule

    def _update_schedule_with_dataset(
        self, dataset: str, schedule: str, is_default_schedule: bool
    ) -> Union[Dataset, DatasetOrTimeSchedule]:
        """Caso informado um dataset o schedule é alterado
        para ser condicionado a execução por Dataset ou
        DatasetOrTimeSchedule
        (caso o valor de schedule esteja informado no YAML).
        """
        if not is_default_schedule:
            return DatasetOrTimeSchedule(
                timetable=CronTriggerTimetable(
                    schedule, timezone=os.getenv("AIRFLOW__CORE__DEFAULT_TIMEZONE")
                ),
                datasets=[Dataset(dataset)],
            )
        return [Dataset(dataset)]

    def _update_schedule(
        self, specs: DAGConfig
    ) -> Union[str, Union[Dataset, DatasetOrTimeSchedule]]:
        """Atualiza o valor do schedule para o
        valor default ou para Dataset, se for o caso.
        """
        schedule = specs.schedule

        if schedule is None:
            schedule = self._get_safe_schedule(
                specs=specs,
                default_schedule=self.DEFAULT_SCHEDULE
            )
            is_default_schedule = True
        else:
            is_default_schedule = False

        if specs.dataset is not None:
            schedule = self._update_schedule_with_dataset(
                dataset=specs.dataset,
                schedule=schedule,
                is_default_schedule=is_default_schedule,
            )

        return schedule

    def generate_dags(self):
        """Iterates over the YAML files and creates all dags"""

        files_list = []

        for directory in self.YAMLS_DIR_LIST:
            for dirpath, _, filenames in os.walk(directory):
                for filename in filenames:
                    if any(ext in filename for ext in [".yaml", ".yml"]):
                        files_list.extend([os.path.join(dirpath, filename)])

        for filepath in files_list:
            dag_specs = self.parser(filepath).parse()
            dag_id = dag_specs.id
            globals()[dag_id] = self.create_dag(dag_specs, filepath)

    def perform_searches(
        self,
        header,
        sources,
        territory_id,
        term_list,
        dou_sections: List[str],
        search_date,
        field,
        is_exact_search: Optional[bool],
        ignore_signature_match: Optional[bool],
        force_rematch: Optional[bool],
        full_text: Optional[bool],
        use_summary: Optional[bool],
        result_as_email: Optional[bool],
        department: List[str],
        **context,
    ) -> dict:
        """Performs the search in each source and merge the results"""
        logging.info("Searching for: %s", term_list)
        logging.info("Trigger date: %s", get_trigger_date(context, local_time=True))

        if "DOU" in sources:
            dou_result = self.searchers["DOU"].exec_search(
                term_list,
                dou_sections,
                search_date,
                field,
                is_exact_search,
                ignore_signature_match,
                force_rematch,
                department,
                get_trigger_date(context, local_time=True),
            )
        elif "INLABS" in sources:
            inlabs_result = self.searchers["INLABS"].exec_search(
                term_list,
                dou_sections,
                search_date,
                department,
                ignore_signature_match,
                full_text,
                use_summary,
                get_trigger_date(context, local_time=True),
            )

        if "QD" in sources:
            qd_result = self.searchers["QD"].exec_search(
                territory_id,
                term_list,
                dou_sections,
                search_date,
                field,
                is_exact_search,
                ignore_signature_match,
                force_rematch,
                get_trigger_date(context, local_time=True),
                result_as_email,
            )

        if "DOU" in sources and "QD" in sources:
            result = merge_results(qd_result, dou_result)
        elif "INLABS" in sources and "QD" in sources:
            result = merge_results(qd_result, inlabs_result)
        elif "DOU" in sources:
            result = dou_result
        elif "INLABS" in sources:
            result = inlabs_result
        else:
            result = qd_result

        # Add more specs info
        search_dict = {}
        search_dict["result"] = result
        search_dict["header"] = header
        search_dict["department"] = department

        return search_dict

    def has_matches(self, search_result: list, skip_null: bool) -> str:

        if skip_null:
            skip_notification = True
            search_result = ast.literal_eval(search_result)
            for search in search_result:
                items = ["contains" for k, v in search["result"].items() if v]
                if items:
                    skip_notification = False
            return "skip_notification" if skip_notification else "send_notification"
        else:
            return "send_notification"

    def select_terms_from_db(self, sql: str, conn_id: str):
        """Queries the `sql` and return the list of terms that will be
        used later in the DOU search. The first column of the select
        must contain the terms to be searched. The second column, which
        is optional, is a classifier that will be used to group and sort
        the email report and the generated CSV.
        """
        conn_type = BaseHook.get_connection(conn_id).conn_type
        if conn_type == "mssql":
            db_hook = MsSqlHook(conn_id)
        elif conn_type in ("postgresql", "postgres"):
            db_hook = PostgresHook(conn_id)
        else:
            raise Exception("Tipo de banco de dados não suportado: ", conn_type)

        terms_df = db_hook.get_pandas_df(sql)
        # Remove unnecessary spaces and change null for ''
        terms_df = terms_df.applymap(lambda x: str.strip(x) if pd.notnull(x) else "")

        return terms_df.to_json(orient="columns")

    def create_dag(self, specs: DAGConfig, config_file: str) -> DAG:
        """Creates the DAG object and tasks

        Depending on configuration it adds an extra prior task to query
        the term_list from a database
        """
        # Prepare the markdown documentation
        doc_md = (
            self.prepare_doc_md(specs, config_file) if specs.doc_md else None
        )
        # DAG parameters
        default_args = {
            "owner": specs.owner,
            "start_date": datetime(2021, 10, 18),
            "depends_on_past": False,
            "retries": 10,
            "retry_delay": timedelta(minutes=20),
            "on_retry_callback": self.on_retry_callback,
            "on_failure_callback": self.on_failure_callback,
        }

        schedule = self._update_schedule(specs)

        dag = DAG(
            specs.id,
            default_args=default_args,
            schedule=schedule,
            description=specs.description,
            doc_md=doc_md,
            catchup=False,
            params={"trigger_date": "2022-01-02T12:00"},
            tags=specs.tags,
        )

        with dag:

            with TaskGroup(group_id="exec_searchs") as tg_exec_searchs:
                counter = 0
                for subsearch in specs.search:
                    counter += 1
                    if subsearch["sql"]:
                        select_terms_from_db_task = PythonOperator(
                            task_id=f"select_terms_from_db_{counter}",
                            python_callable=self.select_terms_from_db,
                            op_kwargs={
                                "sql": subsearch["sql"],
                                "conn_id": subsearch["conn_id"],
                            },
                        )
                    term_list = (
                        "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_db_"
                        + str(counter)
                        + "') }}"
                    )

                    exec_search_task = PythonOperator(
                        task_id=f"exec_search_{counter}",
                        python_callable=self.perform_searches,
                        op_kwargs={
                            "header": subsearch["header"],
                            "sources": subsearch["sources"],
                            "territory_id": subsearch["territory_id"],
                            "term_list": subsearch["terms"] or term_list,
                            "dou_sections": subsearch["dou_sections"],
                            "search_date": subsearch["search_date"],
                            "field": subsearch["field"],
                            "is_exact_search": subsearch["is_exact_search"],
                            "ignore_signature_match": subsearch[
                                "ignore_signature_match"
                            ],
                            "force_rematch": subsearch["force_rematch"],
                            "full_text": subsearch["full_text"],
                            "use_summary": subsearch["use_summary"],
                            "department": subsearch["department"],
                            "result_as_email": result_as_html(specs),
                        },
                    )

                    if subsearch["sql"]:
                        # pylint: disable=pointless-statement
                        select_terms_from_db_task >> exec_search_task

            has_matches_task = BranchPythonOperator(
                task_id="has_matches",
                python_callable=self.has_matches,
                op_kwargs={
                    "search_result": "{{ ti.xcom_pull(task_ids="
                    + str(
                        [
                            f"exec_searchs.exec_search_{count + 1}"
                            for count in range(counter)
                        ]
                    )
                    + ") }}",
                    "skip_null": specs.report.skip_null,
                },
            )

            skip_notification_task = EmptyOperator(task_id="skip_notification")

            send_notification_task = PythonOperator(
                task_id="send_notification",
                python_callable=Notifier(specs).send_notification,
                op_kwargs={
                    "search_report": "{{ ti.xcom_pull(task_ids="
                    + str(
                        [
                            f"exec_searchs.exec_search_{count + 1}"
                            for count in range(counter)
                        ]
                    )
                    + ") }}",
                    "report_date": template_ano_mes_dia_trigger_local_time,
                },
            )

            # pylint: disable=pointless-statement
            tg_exec_searchs >> has_matches_task

            has_matches_task >> [send_notification_task, skip_notification_task]

        return dag


# # Run dag generation
DouDigestDagGenerator().generate_dags()
